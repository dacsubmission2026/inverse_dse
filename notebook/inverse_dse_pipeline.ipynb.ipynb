{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c0a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:01:36.311854Z",
     "iopub.status.busy": "2025-11-17T23:01:36.311524Z",
     "iopub.status.idle": "2025-11-18T01:24:03.995996Z",
     "shell.execute_reply": "2025-11-18T01:24:03.994883Z"
    },
    "papermill": {
     "duration": 8547.69578,
     "end_time": "2025-11-18T01:24:03.998216",
     "exception": false,
     "start_time": "2025-11-17T23:01:36.302436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_space = {\n",
    "    \"cpu_clock_GHz\": np.round(np.linspace(0.3, 3.0, 24), 2),\n",
    "    \"l1i_kb\": 2 ** np.arange(4, 9),  \n",
    "    \"l1d_kb\": 2 ** np.arange(4, 9),\n",
    "    \"l1_assoc\": 2 ** np.arange(0, 4), \n",
    "    \"l2_kb\": 2 ** np.arange(7, 12),  \n",
    "    \"l2_assoc\": 2 ** np.arange(1, 5), \n",
    "    \"fetchWidth\": np.arange(4, 13, 1, dtype=int), \n",
    "    \"decodeWidth\": np.arange(4, 13, 1, dtype=int),\n",
    "    \"renameWidth\": np.arange(4, 13, 1, dtype=int),\n",
    "    \"dispatchWidth\": np.arange(4, 13, 1, dtype=int),\n",
    "    \"issueWidth\": np.arange(4, 13, 1, dtype=int),\n",
    "    \"commitWidth\": np.arange(4, 13, 1, dtype=int),\n",
    "    \"wbWidth\": np.arange(6, 13, 1, dtype=int),\n",
    "    \"numROBEntries\": np.arange(32, 257, 16, dtype=int),\n",
    "    \"numIQEntries\": np.arange(16, 129, 16, dtype=int),\n",
    "    \"numPhysIntRegs\": np.arange(64, 513, 32, dtype=int),\n",
    "    \"numPhysFloatRegs\": np.arange(64, 513, 32, dtype=int),\n",
    "    \"LQEntries\": np.arange(8, 65, 8, dtype=int),\n",
    "    \"SQEntries\": np.arange(8, 65, 8, dtype=int),\n",
    "    \"branch_predictor\": [\n",
    "        \"BiModeBP\", \"LocalBP\", \"TAGE\", \"TAGE_SC_L_64KB\",\n",
    "        \"MultiperspectivePerceptron64KB\", \"TournamentBP\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "classification_targets = [\n",
    "    'l1i_kb', 'l1d_kb', 'l2_kb', \n",
    "    'branch_predictor'\n",
    "]\n",
    "\n",
    "regression_targets = [\n",
    "    'fetchWidth', 'decodeWidth', 'renameWidth', 'dispatchWidth',\n",
    "    'issueWidth', 'commitWidth', 'wbWidth',\n",
    "    'numROBEntries', 'numIQEntries',\n",
    "    'numPhysIntRegs', 'numPhysFloatRegs',\n",
    "    'LQEntries', 'SQEntries', 'cpu_clock_GHz'\n",
    "]\n",
    "\n",
    "all_targets = classification_targets + regression_targets\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/gem5-results/gem5_mcpat_stats_results_cleaned_50k_v3.csv\")\n",
    "df_original = df.copy()\n",
    "\n",
    "feature_columns = [\n",
    "    'workload',\n",
    "    'Area', 'Peak Power', 'Total Leakage', 'Peak Dynamic',\n",
    "    'Subthreshold Leakage', 'Gate Leakage', 'Runtime Dynamic',\n",
    "    'ipc', 'branch_misprediction_rate',\n",
    "    'icache_miss_rate', 'dcache_read_miss_rate', 'dcache_write_miss_rate'\n",
    "]\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "le_workload = LabelEncoder()\n",
    "df[\"workload\"] = le_workload.fit_transform(df[\"workload\"])\n",
    "encoders[\"workload\"] = le_workload\n",
    "\n",
    "classification_encoded = {}\n",
    "for col in classification_targets:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "    classification_encoded[col] = len(le.classes_)\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} classes (CLASSIFICATION)\")\n",
    "\n",
    "\n",
    "X = df[feature_columns]\n",
    "y_class = df[classification_targets]\n",
    "y_reg = df[regression_targets].astype(float)\n",
    "\n",
    "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X, y_class, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "categorical_features = ['workload']\n",
    "continuous_features = [c for c in feature_columns if c not in categorical_features]\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "X_train[continuous_features] = scaler_features.fit_transform(X_train[continuous_features])\n",
    "X_test[continuous_features] = scaler_features.transform(X_test[continuous_features])\n",
    "\n",
    "scaler_targets = StandardScaler()\n",
    "y_reg_train_scaled = pd.DataFrame(\n",
    "    scaler_targets.fit_transform(y_reg_train),\n",
    "    columns=y_reg_train.columns,\n",
    "    index=y_reg_train.index\n",
    ")\n",
    "y_reg_test_scaled = pd.DataFrame(\n",
    "    scaler_targets.transform(y_reg_test),\n",
    "    columns=y_reg_test.columns,\n",
    "    index=y_reg_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b80bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_constraint_samples(n_samples=50000):\n",
    "    print(f\"\\nGenerating {n_samples} random performance constraint scenarios...\")\n",
    "    \n",
    "    samples = {}\n",
    "    \n",
    "    samples['workload'] = np.random.choice(\n",
    "        X_train.reset_index(drop=True)['workload'].unique(),\n",
    "        n_samples\n",
    "    )\n",
    "    \n",
    "    for col in continuous_features:\n",
    "        col_original = df_original[col]\n",
    "        mean, std = col_original.mean(), col_original.std()\n",
    "        min_val, max_val = col_original.min(), col_original.max()\n",
    "        \n",
    "        samples[col] = np.random.normal(mean, std, n_samples)\n",
    "        samples[col] = np.clip(samples[col], min_val, max_val)\n",
    "    \n",
    "    df_samples = pd.DataFrame(samples)\n",
    "    print(f\"Generated {len(df_samples)} samples\")\n",
    "    return df_samples\n",
    "\n",
    "def predict_hardware_configs(model, df_samples, scaler_features, scaler_targets, encoders):\n",
    "    print(\"\\nü§ñ Predicting hardware configurations...\")\n",
    "    \n",
    "    X_cats = df_samples[categorical_features].values\n",
    "    X_conts = scaler_features.transform(df_samples[continuous_features])\n",
    "    \n",
    "    X_cats_t = torch.tensor(X_cats, dtype=torch.long).to(device)\n",
    "    X_conts_t = torch.tensor(X_conts, dtype=torch.float32).to(device)\n",
    "    \n",
    "    temp_dataset = torch.utils.data.TensorDataset(X_cats_t, X_conts_t)\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    all_class_preds = []\n",
    "    all_reg_preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for cats, conts in tqdm(temp_loader, desc=\"Predicting\", leave=False):\n",
    "            class_outputs, reg_output = model(cats, conts)\n",
    "            \n",
    "            class_preds = [torch.argmax(out, dim=1).cpu().numpy() for out in class_outputs]\n",
    "            all_class_preds.append(np.stack(class_preds, axis=1))\n",
    "            \n",
    "            reg_preds = scaler_targets.inverse_transform(reg_output.cpu().numpy())\n",
    "            all_reg_preds.append(reg_preds)\n",
    "    \n",
    "    y_class_pred = np.vstack(all_class_preds)\n",
    "    y_reg_pred = np.vstack(all_reg_preds)\n",
    "    \n",
    "    df_class = pd.DataFrame(y_class_pred, columns=classification_targets)\n",
    "    for col in classification_targets:\n",
    "        df_class[col] = encoders[col].inverse_transform(df_class[col])\n",
    "    \n",
    "    df_reg = pd.DataFrame(y_reg_pred, columns=regression_targets)\n",
    "    for i, col in enumerate(regression_targets):\n",
    "        df_reg[col] = np.clip(\n",
    "            np.round(df_reg[col]),\n",
    "            param_space[col].min(),\n",
    "            param_space[col].max()\n",
    "        ).astype(int)\n",
    "    \n",
    "    df_hardware = pd.concat([df_class, df_reg], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Predicted {len(df_hardware)} hardware configurations\")\n",
    "    return df_hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predicted_configs(df_hardware, param_space):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATING PREDICTED HARDWARE CONFIGURATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    violations = 0\n",
    "    \n",
    "    for col in df_hardware.columns:\n",
    "        if col in param_space:\n",
    "            valid_values = param_space[col]\n",
    "            \n",
    "            if df_hardware[col].dtype in ['float64', 'float32']:\n",
    "                df_hardware[col] = df_hardware[col].round(2)\n",
    "            \n",
    "            invalid_mask = ~df_hardware[col].isin(valid_values)\n",
    "            \n",
    "            if invalid_mask.any():\n",
    "                violations += invalid_mask.sum()\n",
    "                print(f\"{col}: {invalid_mask.sum()} invalid predictions\")\n",
    "    \n",
    "    if violations == 0:\n",
    "        print(\"ALL PREDICTIONS ARE VALID!\")\n",
    "        print(f\"   {len(df_hardware)} configs meet parameter space constraints\")\n",
    "    else:\n",
    "        print(f\"\\nFound {violations} constraint violations\")\n",
    "    \n",
    "    return violations == 0\n",
    "\n",
    "def apply_constraints_and_optimize(df_samples, df_hardware, constraints, objective, top_k=10):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING CONSTRAINTS AND OPTIMIZING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    df_combined = pd.concat([\n",
    "        df_samples.reset_index(drop=True),\n",
    "        df_hardware.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    mask = pd.Series([True] * len(df_combined))\n",
    "    \n",
    "    print(\"Constraints:\")\n",
    "    for col, (min_val, max_val) in constraints.items():\n",
    "        if col in df_combined.columns:\n",
    "            if min_val is not None:\n",
    "                mask &= (df_combined[col] >= min_val)\n",
    "                print(f\"  {col} >= {min_val}\")\n",
    "            if max_val is not None:\n",
    "                mask &= (df_combined[col] <= max_val)\n",
    "                print(f\"  {col} <= {max_val}\")\n",
    "    \n",
    "    df_feasible = df_combined[mask].copy()\n",
    "    \n",
    "    if df_feasible.empty:\n",
    "        print(\"\\nNO FEASIBLE DESIGNS FOUND!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nFound {len(df_feasible)} feasible designs ({len(df_feasible)/len(df_combined)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nOptimizing for: {objective}\")\n",
    "    \n",
    "    if objective == 'min_power':\n",
    "        df_feasible['objective'] = df_feasible['Peak Power']\n",
    "        ascending = True\n",
    "    elif objective == 'min_area':\n",
    "        df_feasible['objective'] = df_feasible['Area']\n",
    "        ascending = True\n",
    "    elif objective == 'max_ipc':\n",
    "        df_feasible['objective'] = df_feasible['ipc']\n",
    "        ascending = False\n",
    "    elif objective == 'pca':\n",
    "        df_feasible['objective'] = (df_feasible['Peak Power'] * df_feasible['Area']) / (df_feasible['ipc'] + 1e-6)\n",
    "        ascending = True\n",
    "    elif objective == 'max_perf_per_watt':\n",
    "        df_feasible['objective'] = df_feasible['ipc'] / (df_feasible['Peak Power'] + 1e-6)\n",
    "        ascending = False\n",
    "    else:\n",
    "        print(f\"Unknown objective: {objective}\")\n",
    "        return None\n",
    "    \n",
    "    df_top = df_feasible.sort_values('objective', ascending=ascending).head(top_k)\n",
    "    \n",
    "    return df_top\n",
    "\n",
    "def display_top_designs(df_top, objective_name):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP {len(df_top)} RECOMMENDED HARDWARE DESIGNS\")\n",
    "    print(f\"Optimized for: {objective_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df_top.iterrows()):\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"DESIGN OPTION #{i+1}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        print(\"\\nPerformance Requirements:\")\n",
    "        perf_cols = ['ipc', 'Peak Power', 'Area', 'Runtime Dynamic',\n",
    "                     'branch_misprediction_rate', 'icache_miss_rate']\n",
    "        for col in perf_cols:\n",
    "            if col in row:\n",
    "                print(f\"  {col:30s}: {row[col]:.4f}\")\n",
    "        \n",
    "        print(\"\\nRecommended Hardware Configuration:\")\n",
    "        for col in all_targets:\n",
    "            if col in row:\n",
    "                print(f\"  {col:30s}: {row[col]}\")\n",
    "        \n",
    "        print(f\"\\nObjective Value: {row['objective']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCPUDataset(Dataset):\n",
    "    def __init__(self, X, y_class, y_reg, cat_cols, cont_cols):\n",
    "        self.cats = torch.tensor(X[cat_cols].values, dtype=torch.long)\n",
    "        self.conts = torch.tensor(X[cont_cols].values, dtype=torch.float32)\n",
    "        self.y_class = torch.tensor(y_class.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cats)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.cats[idx], self.conts[idx], self.y_class[idx], self.y_reg[idx]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    HybridCPUDataset(X_train, y_class_train, y_reg_train_scaled, \n",
    "                     categorical_features, continuous_features),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    HybridCPUDataset(X_test, y_class_test, y_reg_test_scaled,\n",
    "                     categorical_features, continuous_features),\n",
    "    batch_size=256, shuffle=False\n",
    ")\n",
    "\n",
    "class HybridTabTransformer(nn.Module):\n",
    "    def __init__(self, categories, num_continuous, \n",
    "                 classification_dims, num_regression_outputs,\n",
    "                 dim=128, depth=4, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_cont = num_continuous\n",
    "        \n",
    "        self.cat_embeds = nn.ModuleList([\n",
    "            nn.Embedding(cat_size, dim) for cat_size in categories\n",
    "        ])\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim, nhead=heads, dropout=dropout, batch_first=True\n",
    "            ),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        \n",
    "        self.continuous_mlp = nn.Sequential(\n",
    "            nn.Linear(num_continuous, dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim * 2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 2, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.classification_heads = nn.ModuleList([\n",
    "            nn.Linear(128, out_dim) for out_dim in classification_dims\n",
    "        ])\n",
    "        \n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_regression_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cats, x_conts):\n",
    "        emb = torch.stack([\n",
    "            emb_layer(x_cats[:, i])\n",
    "            for i, emb_layer in enumerate(self.cat_embeds)\n",
    "        ], dim=1)\n",
    "        trans_out = self.transformer(emb)\n",
    "        trans_out = trans_out.mean(dim=1)\n",
    "        \n",
    "        cont_out = self.continuous_mlp(x_conts)\n",
    "        combined = torch.cat([trans_out, cont_out], dim=1)\n",
    "        shared = self.post_mlp(combined)\n",
    "        \n",
    "        class_outputs = [head(shared) for head in self.classification_heads]\n",
    "        \n",
    "        reg_output = self.regression_head(shared)\n",
    "        \n",
    "        return class_outputs, reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c43159",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "categories = [int(X_train[col].nunique()) for col in categorical_features]\n",
    "classification_dims = [classification_encoded[col] for col in classification_targets]\n",
    "\n",
    "print(f\"\\nInput categorical features: {categorical_features}\")\n",
    "print(f\"Categorical cardinalities: {categories}\")\n",
    "print(f\"\\nClassification outputs: {classification_targets}\")\n",
    "print(f\"Classification dimensions: {classification_dims}\")\n",
    "print(f\"\\nRegression outputs: {regression_targets}\")\n",
    "print(f\"Regression dimension: {len(regression_targets)}\")\n",
    "\n",
    "model = HybridTabTransformer(\n",
    "    categories=categories,\n",
    "    num_continuous=len(continuous_features),\n",
    "    classification_dims=classification_dims,\n",
    "    num_regression_outputs=len(regression_targets),\n",
    "    dim=128,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "epochs = 300\n",
    "patience = 13\n",
    "best_loss = np.inf\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    for cats, conts, y_class, y_reg in pbar:\n",
    "        cats = cats.to(device)\n",
    "        conts = conts.to(device)\n",
    "        y_class = y_class.to(device)\n",
    "        y_reg = y_reg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        class_outputs, reg_output = model(cats, conts)\n",
    "        \n",
    "        loss_class = sum(\n",
    "            criterion_ce(out, y_class[:, i])\n",
    "            for i, out in enumerate(class_outputs)\n",
    "        )\n",
    "        \n",
    "        loss_reg = criterion_mse(reg_output, y_reg)\n",
    "        \n",
    "        loss = 5 * loss_class + loss_reg\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cats, conts, y_class, y_reg in test_loader:\n",
    "            cats = cats.to(device)\n",
    "            conts = conts.to(device)\n",
    "            y_class = y_class.to(device)\n",
    "            y_reg = y_reg.to(device)\n",
    "            \n",
    "            class_outputs, reg_output = model(cats, conts)\n",
    "            \n",
    "            loss_class = sum(\n",
    "                criterion_ce(out, y_class[:, i])\n",
    "                for i, out in enumerate(class_outputs)\n",
    "            )\n",
    "            loss_reg = criterion_mse(reg_output, y_reg)\n",
    "            loss = loss_class + loss_reg\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch < 5:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"hybrid_tabtransformer_best.pt\")\n",
    "        joblib.dump({\n",
    "            'scaler_features': scaler_features,\n",
    "            'scaler_targets': scaler_targets,\n",
    "            'encoders': encoders\n",
    "        }, \"hybrid_preprocessors.pkl\")\n",
    "        if epoch > 0:\n",
    "            print(f\"‚úÖ Best model saved at epoch {epoch+1} (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(\"hybrid_tabtransformer_best.pt\", weights_only=True))\n",
    "preprocessors = joblib.load(\"hybrid_preprocessors.pkl\")\n",
    "scaler_features = preprocessors['scaler_features']\n",
    "scaler_targets = preprocessors['scaler_targets']\n",
    "encoders = preprocessors['encoders']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_class_preds, all_class_true = [], []\n",
    "all_reg_preds, all_reg_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cats, conts, y_class, y_reg in test_loader:\n",
    "        cats, conts = cats.to(device), conts.to(device)\n",
    "        class_outputs, reg_output = model(cats, conts)\n",
    "        \n",
    "        class_preds = [torch.argmax(out, dim=1).cpu().numpy() for out in class_outputs]\n",
    "        all_class_preds.append(np.stack(class_preds, axis=1))\n",
    "        all_class_true.append(y_class.numpy())\n",
    "        \n",
    "        reg_preds = scaler_targets.inverse_transform(reg_output.cpu().numpy())\n",
    "        reg_true = scaler_targets.inverse_transform(y_reg.numpy())\n",
    "        all_reg_preds.append(reg_preds)\n",
    "        all_reg_true.append(reg_true)\n",
    "\n",
    "y_class_pred = np.vstack(all_class_preds)\n",
    "y_class_true = np.vstack(all_class_true)\n",
    "y_reg_pred = np.vstack(all_reg_preds)\n",
    "y_reg_true = np.vstack(all_reg_true)\n",
    "\n",
    "for i, col in enumerate(regression_targets):\n",
    "    y_reg_pred[:, i] = np.clip(\n",
    "        np.round(y_reg_pred[:, i]),\n",
    "        param_space[col].min(),\n",
    "        param_space[col].max()\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION TARGETS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Parameter':<25} {'Accuracy':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, col in enumerate(classification_targets):\n",
    "    acc = accuracy_score(y_class_true[:, i], y_class_pred[:, i])\n",
    "    print(f\"{col:<25} {acc:<12.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGRESSION TARGETS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Parameter':<25} {'MAE':<12} {'MAE%':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, col in enumerate(regression_targets):\n",
    "    mae = mean_absolute_error(y_reg_true[:, i], y_reg_pred[:, i])\n",
    "    \n",
    "    valid_range = param_space[col]\n",
    "    valid_min = valid_range.min()\n",
    "    valid_max = valid_range.max()\n",
    "    mae_p = mae / (valid_max - valid_min)\n",
    "    \n",
    "    print(f\"{col:<25} {mae:<12.2f} {mae_p:<15.1%}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nHYBRID MODEL SUCCESSFULLY TRAINED!\")\n",
    "print(\"   - Classification: Discrete/categorical parameters\")\n",
    "print(\"   - Regression: Integer range parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03b3fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:24:27.266423Z",
     "iopub.status.busy": "2025-11-18T01:24:27.265912Z",
     "iopub.status.idle": "2025-11-18T01:24:28.027145Z",
     "shell.execute_reply": "2025-11-18T01:24:28.026049Z"
    },
    "papermill": {
     "duration": 12.22695,
     "end_time": "2025-11-18T01:24:28.028878",
     "exception": false,
     "start_time": "2025-11-18T01:24:15.801928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_samples = generate_constraint_samples(n_samples=10000)\n",
    "\n",
    "df_hardware = predict_hardware_configs(\n",
    "    model, df_samples, scaler_features, scaler_targets, encoders\n",
    ")\n",
    "\n",
    "validate_predicted_configs(df_hardware, param_space)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE : High-Performance Power-Efficient Design\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "constraints_1 = {\n",
    "    'ipc': (1.5, None),           # IPC >= 1.5\n",
    "    'Peak Power': (None, 50.0),   # Power <= 50W\n",
    "    'Area': (None, 200.0)         # Area <= 200 mm¬≤\n",
    "}\n",
    "\n",
    "df_top_1 = apply_constraints_and_optimize(\n",
    "    df_samples, df_hardware,\n",
    "    constraints=constraints_1,\n",
    "    objective='max_perf_per_watt',\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "if df_top_1 is not None:\n",
    "    display_top_designs(df_top_1, 'Max Performance per Watt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8567589,
     "sourceId": 13770312,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8895.378395,
   "end_time": "2025-11-18T01:29:47.150470",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T23:01:31.772075",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
